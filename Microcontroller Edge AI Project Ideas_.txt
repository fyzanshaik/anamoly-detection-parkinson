A Blueprint for Advanced Embedded Systems: Designing a Distributed, Explainable Anomaly Detection Network for Predictive Maintenance


Part I: Foundational Concepts for the Modern Embedded Engineer


Section 1: The Paradigm Shift to the Intelligent Edge


The field of embedded systems is undergoing its most significant transformation in a generation. The convergence of ubiquitous connectivity, powerful yet low-cost microcontrollers, and breakthroughs in artificial intelligence has given rise to a new architectural paradigm: edge computing. This section establishes the foundational principles of this shift, moving from the traditional cloud-centric model to the intelligent, autonomous processing capabilities of the "Tiny Edge." Understanding this evolution is paramount, as it provides the essential context and justification for the advanced project detailed in this report. It explains not just the "what" but, more critically, the "why" behind designing systems that are intelligent, responsive, and trustworthy right where data is born.


1.1 Defining the Edge: From Cloud to Microcontroller


For years, the dominant architecture for the Internet of Things (IoT) has been a centralized, cloud-based model. In this paradigm, embedded devices act as simple data collectors, sensing their environment and streaming raw data to powerful cloud servers for processing, analysis, and storage. While this model has enabled vast data aggregation and complex analytics, its inherent limitations become starkly apparent in applications that demand real-time responsiveness, operational resilience, and data privacy.1
The primary drawbacks of a purely cloud-dependent approach include significant latency, high bandwidth consumption, and security vulnerabilities. Latency, the delay in transmitting data to the cloud and receiving a response, can be prohibitive in time-critical systems. Consider a smart traffic light system; if it must send video feeds to a remote server to analyze traffic flow and make a decision, the round-trip delay could render the system ineffective or even dangerous.3 Similarly, continuously streaming high-volume data, such as vibration or audio from industrial machinery, consumes enormous network bandwidth, leading to substantial operational costs.3 Furthermore, transmitting sensitive operational or personal data to a central server introduces privacy concerns and expands the attack surface for potential security breaches.1
Edge computing emerges as the direct solution to these challenges. It is a distributed computing paradigm that fundamentally relocates computation and data storage closer to the sources of data generation.1 Instead of a passive sensor sending data to a remote brain, the edge device itself becomes capable of local processing and intelligent decision-making. This architectural shift yields profound benefits:
* Reduced Latency: By processing data on or near the device, response times can be reduced to milliseconds, enabling true real-time control and analysis essential for applications in autonomous vehicles, industrial robotics, and predictive maintenance.3
* Improved Bandwidth Efficiency: Data is filtered, aggregated, and analyzed locally. Only essential insights, summaries, or alerts are transmitted to the cloud, drastically reducing network traffic and associated costs.2
* Enhanced Privacy and Security: Sensitive raw data remains on the local device, minimizing exposure to interception during transmission and ensuring greater compliance with data privacy regulations.1
* Increased Reliability: Edge devices can continue to operate intelligently and autonomously even if the connection to the cloud is intermittent or lost, a critical feature for remote or mission-critical deployments.3
It is important to recognize that "the edge" is not a monolith but a spectrum. It includes powerful edge gateways and single-board computers like the Raspberry Pi or NVIDIA Jetson, which possess considerable processing power and memory.6 However, the most ubiquitous and cost-effective frontier of the edge is what can be termed the "Tiny Edge"—the realm of microcontrollers (MCUs). These are the tiny, power-efficient chips embedded by the billions in sensors, actuators, and everyday devices.1 The ultimate goal of edge computing is to imbue these tiny devices with intelligence, a challenge that requires a specialized approach to AI.


1.2 Introducing TinyML: AI on a Power Budget


The ambition of running artificial intelligence on the Tiny Edge confronts a fundamental obstacle: the immense computational and memory requirements of traditional machine learning models versus the severe resource constraints of microcontrollers. A typical MCU may have only a few hundred kilobytes of RAM and Flash memory and a processor running at tens of megahertz, orders of magnitude less than a standard computer or even a smartphone.1 This resource gap has historically prevented the direct deployment of ML models on MCUs.
Tiny Machine Learning (TinyML) is the revolutionary field that bridges this gap. TinyML is the practice of designing and optimizing machine learning models to run on resource-constrained, low-power embedded devices like microcontrollers.6 It is not merely a scaled-down version of ML; it is a specialized discipline focused on deep model optimization and efficient on-device inference. The development of TinyML frameworks is the critical catalyst that makes intelligent processing on the Tiny Edge a practical reality. Without TinyML, the concept of edge AI would be largely confined to more powerful, and therefore more expensive and power-hungry, devices.
The core benefits of TinyML are transformative for embedded systems design 4:
* Extreme Power Efficiency: TinyML models and inference engines are designed to consume milliwatts of power, enabling AI-powered devices to run for months or even years on small batteries or energy harvesting systems.
* Minimal Latency: With the model running directly on the MCU connected to the sensor, inference happens almost instantaneously, eliminating network delays for time-critical decisions.
* Enhanced Privacy: As the data is processed locally, sensitive information, such as audio from a microphone or vibration from a proprietary machine, never has to leave the device.
* Offline Functionality: TinyML-powered devices are fully autonomous, capable of performing their intelligent function without any reliance on a network connection.
The TinyML workflow represents a paradigm shift from traditional embedded programming. Instead of writing explicit, rule-based code for every possible scenario, an engineer trains a model to learn patterns from data. This workflow can be summarized in a few key stages, which will be elaborated upon throughout this report 4:
1. Data Collection: Gathering representative sensor data from the target device and environment.
2. Offline Model Training: Using a standard ML framework like TensorFlow or PyTorch on a powerful computer to train a neural network to perform a specific task (e.g., classification, anomaly detection).
3. Model Optimization: This is the crucial TinyML step. The trained model undergoes a process of quantization, where its 32-bit floating-point parameters are converted to 8-bit integers, and potentially pruning, where insignificant connections in the network are removed. These techniques drastically reduce the model's size and computational complexity with minimal loss in accuracy.4
4. On-Device Deployment and Inference: The optimized model is converted into a specific format, such as a C array, and compiled into the firmware of the microcontroller. The MCU then uses a lightweight inference engine, like TensorFlow Lite for Microcontrollers, to run the model and make predictions using live sensor data.
This project is built upon this modern workflow, demonstrating how to harness the power of TinyML to create a sophisticated, real-world embedded system that is far more capable than what was possible with traditional methods alone.


Section 2: Anomaly Detection on Resource-Constrained Devices


At the heart of many intelligent edge applications, particularly in industrial IoT (IIoT), lies the task of anomaly detection. For predictive maintenance—the practice of identifying potential equipment failures before they occur—anomaly detection is the core enabling technology. This section delves into the theory and practical implementation of anomaly detection specifically tailored for the resource-constrained environment of a microcontroller, laying the groundwork for the project's central machine learning task.


2.1 The Theory of Anomaly Detection


An anomaly is formally defined as a data point or a pattern of data that deviates significantly from the expected or "normal" behavior.9 In the context of monitoring an industrial motor, normal behavior might be a consistent vibration pattern and sound profile during operation. An anomaly could manifest as a new, high-frequency vibration, a sudden increase in current draw, or an unusual acoustic signature, any of which could signal an impending fault like a bearing failure, an unbalanced load, or a lubrication problem.15
A critical challenge in building such systems is the nature of the data itself. In most real-world industrial scenarios, equipment operates normally the vast majority of the time. Failures, and thus anomalous data, are rare, unpredictable, and can manifest in countless different ways.17 It is impractical, if not impossible, to collect a comprehensive dataset that includes examples of every possible failure mode. This reality makes traditional supervised machine learning, which requires labeled examples of all classes (both "normal" and various "anomalous" states), an unsuitable approach.
This data scarcity problem necessitates the use of unsupervised learning. In this paradigm, the machine learning model is trained only on data from the system's normal state of operation. The model learns a deep and robust representation of what "normal" looks like. Subsequently, during inference, any data that does not conform to this learned representation is flagged as an anomaly.10 This approach is powerful because it does not need to have ever seen a specific type of failure before; it only needs to know that the current state is not normal.
Among various unsupervised methods, the autoencoder neural network architecture is exceptionally well-suited for this task. An autoencoder consists of two main parts: an encoder and a decoder.10
* The encoder takes the high-dimensional input data (e.g., features from a vibration sensor) and compresses it down to a much lower-dimensional representation, often called the "bottleneck" or "latent space."
* The decoder takes this compressed representation and attempts to reconstruct the original high-dimensional input data.
The key insight for anomaly detection lies in the training process. The autoencoder is trained exclusively on normal data, and its objective is to minimize the reconstruction error—the difference between the original input and the reconstructed output. Through training, it becomes highly proficient at compressing and then accurately reconstructing normal data patterns. However, when this trained model is presented with anomalous data—a pattern it has never seen before—it struggles to reconstruct it accurately from the compressed representation. This results in a high reconstruction error.10 This reconstruction error, therefore, becomes a direct and effective score for anomalousness. By setting a threshold on this error, the system can reliably distinguish between normal and abnormal states.


2.2 The TinyML Anomaly Detection Pipeline in Detail


The theoretical concept of an autoencoder for anomaly detection must be translated into a practical pipeline that can be executed on a microcontroller. This involves a series of well-defined steps that bridge the gap between raw sensor data and an intelligent on-device decision.
Step 1: Data Collection & Preprocessing
The foundation of any successful ML model is high-quality, representative data. For this project, this means capturing vibration data from the target motor using the actual sensor (e.g., an accelerometer) that will be used in the final deployment.6 This data must be collected during various "normal" operating states (e.g., different speeds, loads). The raw time-series data is then preprocessed. A crucial preprocessing technique is
segmentation, where the continuous stream of data is broken into fixed-size windows (e.g., 2-second segments) using a sliding window approach. This creates individual samples for the model to process.10 Another key step is
normalization, where the data values are scaled to a standard range (e.g., 0 to 1), which helps the neural network train more effectively.19
Step 2: Feature Extraction
While it is possible to feed raw time-series data directly to a model, this is often computationally inefficient and can make it harder for the model to learn relevant patterns. A more effective approach is to perform feature extraction, transforming the data into a more informative representation. For vibration data, the most powerful feature extraction technique is the Fast Fourier Transform (FFT).11 The FFT converts a signal from the time domain (amplitude over time) into the frequency domain (amplitude over frequency). This transformation is invaluable because the frequency spectrum of a motor's vibration often contains clear signatures of its operational health. A normal motor might have distinct peaks at its rotational frequency and its harmonics, while a fault like a worn bearing might introduce new, high-frequency components.20 By feeding the model these frequency-domain features instead of the raw, noisy time-domain signal, the learning task becomes significantly simpler and more robust.
Step 3: Model Training (Offline)
With a preprocessed dataset of frequency-domain features from normal operation, the autoencoder model is built and trained. This is done offline on a PC or cloud platform using a standard framework like TensorFlow with the Keras API.10 The model is trained to minimize the reconstruction error (e.g., Mean Absolute Error or Mean Squared Error) on the normal training data. After training, the model is evaluated on a separate set of normal data (the validation set) to determine a suitable threshold for the reconstruction error. Any error above this threshold will be considered an anomaly.9
Step 4: Model Conversion and Quantization
This step is the core of the TinyML process and is what makes on-device inference possible. The trained TensorFlow model, which exists as a large file with 32-bit floating-point weights, is converted and optimized using the TensorFlow Lite (TFLite) converter.4 During this process,
post-training quantization is applied. This technique converts the model's 32-bit floating-point numbers into much more efficient 8-bit integers. This has two major benefits:
1. Size Reduction: It reduces the model's memory footprint by up to 4x.
2. Performance Increase: Integer arithmetic is significantly faster than floating-point arithmetic on most microcontrollers, leading to lower latency and power consumption.
The result of this step is a highly optimized .tflite model file, small enough to be stored in the flash memory of an MCU.6
Step 5: On-Device Deployment and Inference
The final stage involves integrating the optimized model into the microcontroller's firmware. The .tflite model file is typically converted into a C source file (e.g., a large C array) using a tool like xxd. This C file is then included in the embedded project.10 The firmware uses the
TensorFlow Lite for Microcontrollers library, a lightweight C++ library that acts as the inference engine. The final on-device inference loop is as follows:
   1. Capture a window of raw sensor data.
   2. Perform preprocessing (normalization) and feature extraction (FFT) on the data.
   3. Feed the resulting feature vector to the TFLite interpreter.
   4. The interpreter executes the autoencoder model and produces the reconstructed feature vector.
   5. Calculate the reconstruction error (e.g., Mean Absolute Error) between the input features and the reconstructed output.22
   6. Compare this error to the pre-determined threshold. If the error is higher, flag an anomaly.
This complete pipeline, from sensor to intelligent decision, demonstrates a modern, data-driven approach to embedded systems engineering that is both powerful and efficient.


Section 3: The Imperative of Explainability in AI (XAI)


As artificial intelligence models become more integrated into critical systems, a new and pressing challenge has emerged: the problem of opacity. Many powerful AI models, especially deep neural networks, operate as "black boxes." They can make remarkably accurate predictions, but the internal logic behind their decisions is often inscrutable to human users. In an industrial or safety-critical context, this is unacceptable. A maintenance technician cannot simply trust an alert that says "Anomaly Detected"; they need to understand why the system flagged an issue to diagnose the root cause and take effective corrective action.23
This need for transparency is the driving force behind the field of Explainable AI (XAI). XAI is a set of methods and techniques aimed at making the decisions of AI systems understandable to humans.25 It is not just about satisfying curiosity; it is about building trust, ensuring accountability, and enabling effective human-machine collaboration.28 In the context of this project, integrating XAI transforms the system from a simple detector into a true diagnostic partner. The functional requirement for an explanation is not an optional add-on but a core design principle that has profound implications for the entire system architecture.


3.1 Beyond the Black Box: Why Explainability Matters


The value of an anomaly detection system is not in the detection itself, but in the action it enables. Simply flagging a motor as "anomalous" provides incomplete information. Is the anomaly due to a high-frequency vibration indicative of bearing wear, or a low-frequency imbalance? The required maintenance actions are completely different. Without an explanation, the technician is left to perform a time-consuming manual inspection, negating many of the efficiency gains the AI system was meant to provide.
XAI addresses this by answering the crucial "why" question. By providing insights into which input features most influenced a model's decision, XAI delivers actionable intelligence.23 This has several key benefits in an industrial setting:
   * Actionable Diagnostics: Explanations guide technicians directly to the likely source of the problem, reducing diagnostic time and improving the accuracy of repairs.
   * Building User Trust: Humans are more likely to trust and rely on systems whose reasoning they can understand. An explainable system allows users to verify that the model is making sensible decisions, rather than blindly following an opaque algorithm.25
   * Model Debugging and Refinement: XAI can reveal when a model is relying on spurious or incorrect correlations in the data, allowing engineers to identify biases and improve the model's robustness.
   * Accountability and Safety: In safety-critical applications, being able to audit and understand an AI's decision-making process is essential for regulatory compliance and ensuring safe operation.29


3.2 A Practical Primer on Lightweight XAI Techniques


XAI methods can be broadly categorized, but for the purpose of this project, the most relevant distinction is between model-agnostic and model-specific techniques. Model-agnostic methods are particularly powerful because they can be applied to any machine learning model, treating it as a black box. This provides immense flexibility.31
LIME (Local Interpretable Model-agnostic Explanations):
LIME is a popular model-agnostic technique that explains an individual prediction. Its core intuition is to understand the behavior of a complex model by approximating it with a simple, interpretable model (like a linear regression model) in the "local" vicinity of the prediction being explained.32 The process works as follows:
   1. Take the specific input data point you want to explain (e.g., the FFT features of an anomalous vibration).
   2. Create a new dataset by generating many slight variations of this input, a process called perturbation.
   3. Feed all these perturbed samples to the original black-box model to get their predictions.
   4. Fit a simple, interpretable model (e.g., a linear model) to this new dataset of perturbed samples and their predictions. The samples are weighted based on their proximity to the original data point.
   5. The simple model's parameters (e.g., the coefficients of the linear model) now provide an explanation for the original prediction, indicating which features had the most positive or negative influence.
Because LIME is computationally less intensive than some other methods, it is considered a viable candidate for implementation on more powerful edge devices.26
SHAP (SHapley Additive exPlanations):
SHAP is another model-agnostic method that is grounded in cooperative game theory.35 It explains a prediction by calculating the contribution of each feature, known as the "Shapley value." It does this by considering every possible combination (coalition) of features and evaluating the model's output, ensuring a fair and theoretically sound distribution of importance among the features.36 SHAP is often considered the gold standard for explanation quality due to its consistency and solid theoretical foundation. However, this rigor comes at a very high computational cost, as the number of possible feature combinations grows exponentially. This makes SHAP generally impractical for real-time execution on microcontrollers but a powerful tool for offline model analysis.34
Feature Importance Plots:
The simplest and most lightweight form of explanation is to analyze the model's reliance on its input features. In the context of this project, this means determining which frequency bins from the FFT are most consistently associated with anomalous predictions.38 While this method lacks the nuance of LIME or SHAP in explaining a single event, it can provide valuable global insights into what types of frequency patterns the model considers important for detecting anomalies. This approach is highly suitable for resource-constrained implementation.


3.3 The XAI on the Edge Challenge


The integration of XAI into embedded systems presents a fundamental engineering trade-off. XAI methods, by their nature, often require significant computation and memory—resources that are precisely what edge devices lack.4 This creates a direct tension between the three desirable goals of an edge AI system:
      1. Performance (Accuracy): How well the model performs its task.
      2. Efficiency (Latency & Power): How quickly and with how little energy the model runs.
      3. Interpretability (Explainability): How well the model's decisions can be understood.
Optimizing for one of these often comes at the expense of the others.35 For instance, a more complex and accurate model may be less efficient and harder to explain. The need to provide an explanation (a functional requirement) combined with the high computational cost of XAI methods directly influences the system's design. It becomes clear that attempting to run a sophisticated XAI analysis on the same low-power microcontroller that is performing the initial sensing and inference is not feasible. This constraint forces an architectural decision: the system must be distributed. The task of generating explanations must be offloaded from the resource-starved sensor node to a more capable, centralized gateway node. This illustrates a critical principle: in modern embedded design, advanced software requirements like explainability are no longer just an analysis step but are primary drivers of the physical system architecture.


Part II: Project Blueprint: A Distributed Motor Health Monitoring System




Section 4: System Architecture and Hardware Selection


This section provides the comprehensive blueprint for the project, detailing the overall system architecture and guiding the critical process of hardware selection. The proposed system is not a single device but a distributed network of intelligent nodes, each playing a specific role. This architecture is a direct consequence of the technical requirements established in Part I: the need for localized, low-latency anomaly detection at the sensor level, coupled with the need for centralized, more computationally intensive processing for data aggregation and explainability.


4.1 System Architecture Overview


The proposed system consists of three primary components, forming a hierarchical, distributed network designed for efficiency, scalability, and intelligence.
      * Sensor Nodes: These are the workhorses of the system, deployed directly on the machinery to be monitored (e.g., electric motors). Each sensor node is comprised of a microcontroller and a relevant sensor (e.g., an accelerometer). The primary responsibility of a sensor node is to perform real-time, on-device anomaly detection using a pre-trained TinyML model.10 It continuously captures sensor data, runs the inference pipeline, and determines if the machine's state is "Normal" or "Anomalous." It then communicates this status wirelessly to the central gateway.
      * Gateway Node: This is the central coordinator and intelligence hub of the network. It consists of a single, more powerful microcontroller. Its responsibilities are multifaceted:
      1. Data Aggregation: It listens for status messages from all sensor nodes in the network, maintaining a real-time overview of the health of all monitored assets.2
      2. XAI Processing: When a sensor node reports an anomaly, the gateway takes on the computationally intensive task of generating an explanation. It requests the raw anomalous data from the node and runs an XAI algorithm to determine the likely cause.40
      3. User Interface: The gateway is responsible for presenting the system status and any generated explanations to the user, potentially via a connected display or a web interface.
      * Communication Protocol: The wireless link between the sensor nodes and the gateway is critical. To meet the requirements of low latency, low power, and high reliability without the complexity and overhead of a traditional Wi-Fi network, this project will utilize ESP-NOW. ESP-NOW is a proprietary, connectionless protocol from Espressif that allows direct device-to-device communication.41 This is ideal for a "many-to-one" topology where multiple sensor nodes report to a single gateway.41 For optional cloud connectivity, the gateway can then use a standard protocol like
MQTT to publish alerts and data to a cloud service, but the core, real-time operation of the network remains local and independent of the internet.40
This architecture efficiently distributes the computational load. The lightweight, repetitive task of anomaly detection is handled by the low-power sensor nodes at the extreme edge, while the heavier, sporadic task of explainability is handled by the more capable gateway.


4.2 Hardware Selection Deep Dive


The success of this project hinges on making informed hardware selections that align with the specific roles of the sensor and gateway nodes.
The Sensor:
The choice of sensor determines the type of physical phenomena that can be monitored. For this project's focus on motor health, several options are viable:
         * Accelerometer: This is the primary and most recommended sensor for this project. It measures vibration and acceleration forces, which are rich sources of information about the mechanical health of rotating machinery.45 Inexpensive and widely available Micro-Electro-Mechanical Systems (MEMS) accelerometers, such as the
ADXL345, are excellent choices. They typically communicate via standard I2C or SPI interfaces, making them easy to integrate with most microcontrollers.45 For rapid prototyping, a development board with a built-in Inertial Measurement Unit (IMU), like the
Arduino Nano 33 BLE Sense which includes the LSM9DS1, can eliminate the need for external wiring initially.10 When selecting an accelerometer, key parameters to consider are its
sensitivity (ability to detect small changes), range (maximum measurable acceleration), and interface.45
         * Microphone (Alternative): An alternative or complementary approach is to use a microphone for acoustic anomaly detection. The sound a machine makes can also be a strong indicator of its health.17 A small electret or MEMS microphone can capture the audio signature of the motor.
         * Current Sensor (Alternative): Monitoring the electrical parameters of a motor provides another dimension for analysis. A non-invasive current sensor, like the ACS712 or a current transformer-based sensor like the HW-666, can measure the current being drawn by the motor. Anomalies in current consumption can indicate electrical faults or mechanical strain.48
The Microcontroller:
This is the most critical hardware decision. The chosen microcontroller must be capable of running the TinyML model and, for this distributed system, must support the ESP-NOW communication protocol (which is based on the 802.11 Wi-Fi standard). The following table provides a comparative analysis of the top contenders to guide this choice.
Table 1: Comparative Analysis of Microcontrollers for the Project


Feature
	ESP32
	Arduino Nano 33 BLE Sense
	Raspberry Pi Pico W
	Justification for this Project
	Processor
	Dual-Core Tensilica LX6 @ 240 MHz 51
	Single-Core ARM Cortex-M4F @ 64 MHz 6
	Dual-Core ARM Cortex-M0+ @ 133 MHz 52
	The ESP32 offers the best raw performance, making it highly suitable for the more demanding gateway node. The Pico W's dual-core processor is also a strong performer and more than capable for both node types. The Nano is sufficient for a single node but is the least powerful.
	RAM / Flash
	520 KB SRAM / 4 MB Flash 51
	256 KB SRAM / 1 MB Flash 6
	264 KB SRAM / 2 MB Flash (onboard) 52
	All three have sufficient memory to store and run a typical TinyML autoencoder model. The ESP32's larger RAM provides more headroom for the gateway's tasks of managing multiple nodes and running XAI routines.
	Connectivity
	Integrated Wi-Fi & Bluetooth 51
	Bluetooth LE only 46
	Integrated Wi-Fi & Bluetooth (on 'W' model) 52
	This is a critical differentiator. The project's distributed communication relies on the Wi-Fi-based ESP-NOW protocol. Therefore, the ESP32 and Raspberry Pi Pico W are the only suitable choices. The Nano 33 BLE Sense, lacking Wi-Fi, cannot be used in this distributed architecture without adding an external Wi-Fi module, which would increase complexity.
	Onboard Sensors
	Hall, Temp, Touch 51
	Excellent: 9-axis IMU, Mic, Temp, Humidity, etc. 46
	Temp Sensor 52
	The Nano 33 BLE Sense is ideal for building a rapid, single-node prototype to test the ML pipeline, as no external sensors are needed. For the final distributed system using the ESP32 or Pico W, an external accelerometer must be added.
	Power Consumption
	Moderate to High
	Low
	Low to Moderate 54
	For battery-powered sensor nodes, the Raspberry Pi Pico W and Nano 33 BLE Sense are more power-efficient. The ESP32 is more power-hungry, making it a better fit for the gateway node, which is more likely to have a permanent power source.
	AI/ML Support
	Excellent (TFLM, ESP-IDF, extensive community support)
	Excellent (TFLM, strong integration with Edge Impulse) 46
	Good and rapidly improving (TFLM, C/C++ SDK, MicroPython)
	All three platforms have robust support for TensorFlow Lite for Microcontrollers. The community and available examples are mature for all, with the ESP32 having the longest history in this space.
	Project Recommendation
	Ideal for both Gateway and Sensor Nodes. The combination of high performance and integrated Wi-Fi makes the ESP32 a powerful and convenient all-around choice. Using ESP32s for all nodes simplifies development.
	Excellent for a single-node prototype only. Its rich sensor suite is perfect for learning the ML workflow, but its lack of Wi-Fi makes it unsuitable for the final distributed system.
	Excellent for Sensor Nodes. Its low cost, low power consumption, and capable processor make it a perfect choice for the distributed sensor nodes. It is also a very capable gateway.
	A sophisticated and cost-effective approach would be to use Raspberry Pi Pico W boards for the sensor nodes and a more powerful ESP32 as the central gateway.
	

Section 5: Implementation Phase 1 - The Sensor Node and Anomaly Detection


This section provides a detailed, step-by-step guide for constructing a single, functional sensor node. This node represents the fundamental building block of the distributed network. The process is broken down into four key stages: hardware assembly, offline data science for model creation, and finally, the development of the on-device firmware that performs real-time inference.


5.1 Hardware Setup


The first step is to physically assemble the sensor node. Assuming the use of an ESP32 development board and an external ADXL345 accelerometer, the connection is straightforward. The ADXL345 typically communicates over the I2C protocol, which requires only two data lines (SDA and SCL) in addition to power and ground.
            * Circuit Connections (ESP32 and ADXL345):
            * ESP32 3.3V -> ADXL345 VCC
            * ESP32 GND -> ADXL345 GND
            * ESP32 GPIO 21 (SDA) -> ADXL345 SDA
            * ESP32 GPIO 22 (SCL) -> ADXL345 SCL
A clear circuit diagram will be provided to the student to ensure correct wiring. It is crucial to use the 3.3V output from the ESP32, as applying 5V could damage the sensor.


5.2 Data Acquisition Firmware


Before a model can be trained, data must be collected. A dedicated Arduino sketch is created for this purpose. Its sole function is to initialize the accelerometer and continuously stream its raw X, Y, and Z axis readings over the serial (USB) port to a connected PC.
            * Firmware Logic:
            1. Include necessary libraries (Wire.h for I2C, and the Adafruit ADXL345 library).
            2. In setup(), initialize the serial communication (Serial.begin(115200)).
            3. Initialize the ADXL345 sensor, setting the desired data rate and measurement range.45 A sampling rate of around 500 Hz is a good starting point for monitoring common motor vibrations.56
            4. In loop(), continuously read the acceleration data for all three axes.
            5. Format the data as a comma-separated string (e.g., "x_val,y_val,z_val") and print it to the serial port using Serial.println().13
This firmware turns the sensor node into a simple data logger, ready for the offline data science phase.


5.3 Offline Model Training and Conversion


This phase is performed on a PC using Python and standard data science libraries. It is where the "intelligence" of the system is created. A Jupyter Notebook is an excellent environment for this interactive process.13
            * Data Collection Script: A Python script using the pyserial library will open the serial port connected to the ESP32 and read the incoming lines of data. It will save these readings into CSV files, with separate collection runs for different "normal" operating conditions of the motor (e.g., motor_off.csv, motor_speed1.csv, motor_speed2.csv).13 It is essential to collect several minutes of data for each state to capture natural variations.
            * Feature Extraction with FFT: Once the data is collected, it must be processed.
            1. The Python script will load the raw time-series data from the CSV files.
            2. The data is segmented into windows (e.g., 1024 samples per window).
            3. For each window, the Fast Fourier Transform (FFT) is applied to each axis using libraries like numpy.fft.fft or scipy.fft.fft.20
            4. The output of the FFT is a set of complex numbers representing the magnitude and phase of each frequency component. For this application, only the magnitude is needed, which can be calculated using np.abs().
            5. Since the FFT output is symmetric, only the first half of the results (the positive frequencies) is retained.21 The result is a feature vector for each window, representing the motor's vibration signature in the frequency domain. Visualizing this power spectrum with
matplotlib is a crucial step to confirm that the FFT is capturing distinct patterns for different motor states.
               * Training the Autoencoder:
               1. The extracted FFT features from all "normal" operating states are combined to form the training dataset.
               2. Using TensorFlow/Keras, a simple autoencoder model is defined. A good starting architecture would be an input layer matching the size of the FFT feature vector, followed by one or two Dense hidden layers with progressively fewer neurons (the encoder), a central "bottleneck" layer with very few neurons (e.g., 8 or 16), and then a symmetric set of Dense layers to reconstruct the original input (the decoder).10
               3. The model is compiled with an optimizer (e.g., Adam) and a loss function (e.g., mean_absolute_error).
               4. The model is trained on the normal data for a set number of epochs.
               5. After training, the model is used to calculate reconstruction errors on a validation set (a portion of the normal data that the model was not trained on). The distribution of these errors is analyzed, and a threshold is set (e.g., mean error + 3 standard deviations) that effectively separates normal data from potential anomalies.9
               * Model Conversion:
               1. The trained Keras model is saved and then converted to the TensorFlow Lite format using tf.lite.TFLiteConverter. During this step, post-training integer quantization is enabled to optimize the model for MCU deployment.10
               2. The resulting .tflite file is then converted into a C header file (model.h) using the command-line tool xxd -i model.tflite > model.h. This command creates a large unsigned char array in a C header file, which can be directly included in the Arduino project.13


5.4 On-Device Inference Firmware


This is the final firmware that will run on the deployed sensor node, combining the data acquisition logic with the embedded ML model.
               * Firmware Logic:
               1. Include the necessary libraries: TensorFlowLite.h and the specific headers for the model's operations.
               2. Include the generated model.h file, which contains the autoencoder model as a C array.
               3. In global scope, define the TFLite objects: an error reporter, the model loader, a tensor arena (a block of memory for the model's operations), and the interpreter itself.
               4. In setup(), initialize the accelerometer. Then, load the model from the C array into the interpreter, allocate the necessary tensors in the tensor arena, and check for any errors.
               5. The loop() function contains the core inference logic:
a. Collect a full window of accelerometer samples (e.g., 1024 samples).
b. Perform an in-place FFT on the collected data. Lightweight C/C++ FFT libraries like ArduinoFFT are available for this purpose.
c. Copy the resulting frequency-domain features into the model's input tensor.
d. Invoke the interpreter by calling interpreter->Invoke().
e. Read the reconstructed features from the model's output tensor.
f. Calculate the Mean Absolute Error (MAE) between the input feature vector and the output feature vector.22

g. Compare the calculated MAE to the threshold determined during offline training.
h. If mae > threshold, set a local is_anomalous flag to true. Otherwise, set it to false.
               6. The status determined by this flag will then be sent to the gateway, as detailed in the next section.
This completes the creation of an autonomous, intelligent sensor node capable of monitoring a machine's health in real-time.


Section 6: Implementation Phase 2 - Building the Distributed Network


With a functional standalone sensor node, the next phase is to network multiple nodes together into a cohesive, distributed system. This requires implementing a low-latency, reliable communication protocol. For this project, ESP-NOW is the ideal choice, as it is specifically designed for the type of direct, many-to-one communication required between the sensor nodes and the gateway.


6.1 Introduction to ESP-NOW


ESP-NOW is a wireless communication protocol developed by Espressif Systems that operates on the 2.4 GHz Wi-Fi band but functions at a lower level of the networking stack.42 Unlike standard Wi-Fi, it does not require devices to connect to an Access Point (AP) or go through the lengthy process of association and authentication. Instead, it enables direct, connectionless, peer-to-peer communication between ESP devices.41
Key advantages of ESP-NOW for this project include:
                  * Low Latency: By bypassing the overhead of traditional Wi-Fi protocols and routers, message transmission is extremely fast, with response times often in the millisecond range. This is perfect for sending real-time status updates.42
                  * Low Power Consumption: The protocol is optimized for efficiency, allowing battery-powered sensor nodes to wake, send a message, and return to sleep very quickly, conserving energy.42
                  * Simplified Network Topology: It eliminates the need for a central Wi-Fi router, simplifying deployment in industrial environments where network infrastructure may not be readily available near the machinery. The nodes communicate directly with the gateway.41
                  * Reliability: ESP-NOW includes built-in acknowledgment mechanisms to confirm message delivery, allowing for the implementation of retry logic to handle packet loss.41


6.2 Implementing the Many-to-One Topology


The core of the networking implementation involves programming the sensor nodes as "senders" and the gateway node as a "receiver."
Getting MAC Addresses:
Every ESP32 has a unique MAC (Media Access Control) address burned into its hardware. To establish an ESP-NOW link, the sender must know the MAC address of the receiver. The first step is to run a simple utility sketch on the gateway board to print its MAC address to the serial monitor. This unique address will then be hard-coded into the firmware of all the sensor nodes.41
The Sender (Sensor Node) Code:
The firmware developed in Section 5 is now extended with communication capabilities.
                  1. Include Libraries: Add #include <esp_now.h> and #include <WiFi.h>.
                  2. Define Data Structure: A C struct is created to define the message format. This ensures that both the sender and receiver interpret the data correctly. The structure should contain all necessary information:
C++
typedef struct struct_message {
   int id; // A unique ID for each sensor node (e.g., 1, 2, 3...)
   bool is_anomalous; // The status from the ML model
   float anomaly_score; // The calculated reconstruction error
} struct_message;

This structure is lightweight and efficient to transmit.41
                  3. Gateway MAC Address: The gateway's MAC address is stored in a uint8_t array.
                  4. setup() Function:
                     * Set the device to Wi-Fi station mode: WiFi.mode(WIFI_STA);.
                     * Initialize ESP-NOW: esp_now_init().
                     * Register the gateway as a peer. This involves creating an esp_now_peer_info_t structure, populating it with the gateway's MAC address, setting the channel (typically 0), and disabling encryption for simplicity. The peer is then added with esp_now_add_peer().41
                     5. loop() Function: After the ML inference is complete and the is_anomalous flag is set, the struct_message is populated with the node's ID and the inference results. The message is then sent to the gateway using a single function call: esp_now_send(gatewayAddress, (uint8_t *) &myData, sizeof(myData));.
The Receiver (Gateway Node) Code:
The gateway firmware is designed to listen for and process messages from all sensor nodes.
                     1. Include Libraries and Define Structure: Same as the sender code, ensuring the struct_message is identical.
                     2. OnDataRecv Callback Function: This is the most important part of the receiver code. It is a function that is automatically executed by the ESP-NOW system whenever a message arrives. It receives the sender's MAC address and the incoming data as arguments. The logic inside this function is as follows:
C++
void OnDataRecv(const uint8_t * mac, const uint8_t *incomingData, int len) {
   // Copy the incoming data into our struct
   memcpy(&myData, incomingData, sizeof(myData));

   // Use the ID from the message to identify the sender
   int nodeId = myData.id;

   // Update the status for that specific node
   // (e.g., in a global array of node statuses)
   node_statuses[nodeId - 1] = myData.is_anomalous;

   // Print the received information for debugging
   Serial.printf("Message from Node %d: Status=%s, Score=%.4f\n", 
                 myData.id, 
                 myData.is_anomalous? "ANOMALOUS" : "NORMAL", 
                 myData.anomaly_score);
}

This callback-driven approach is highly efficient, as the gateway doesn't need to actively poll for data.41
                     3. setup() Function:
                        * Initialize Wi-Fi in station mode and initialize ESP-NOW.
                        * Register the OnDataRecv function as the receive callback: esp_now_register_recv_cb(OnDataRecv);. From this point on, the gateway will automatically process any incoming ESP-NOW messages.


6.3 Ensuring Robust Communication


For a production-grade system, ensuring communication reliability is key. ESP-NOW provides a mechanism for this via a send callback function.
                        * OnDataSent Callback: A function can be registered on the sender node using esp_now_register_send_cb(). This function is called after a transmission attempt and reports whether the delivery was successful or failed.
                        * Retry Logic: Inside this callback, if the status indicates a failure, the sender can implement a simple retry mechanism, attempting to re-send the message a few times before giving up. This handles transient interference or packet loss, making the network more robust.41
For applications covering very large physical areas, the ESP-NOW protocol can also be configured for multi-hop networking, where some nodes are designated as "relays." These relays forward messages from distant nodes to the gateway, effectively extending the range of the network without requiring additional infrastructure.40 While beyond the initial scope of this project, it represents a clear path for future extension.


Section 7: Implementation Phase 3 - Integrating Explainable AI (XAI) at the Gateway


This final implementation phase introduces the most innovative aspect of the project: making the anomaly detection system explainable. As established, simply flagging an anomaly is insufficient for practical use. The system must provide insight into why a particular event was deemed anomalous. This phase details a practical strategy for integrating XAI into the distributed network, addressing the significant computational challenges involved.


7.1 Architectural Decision: Offloading XAI


The first and most critical decision is where the XAI computation will occur. As analyzed in Section 3, running complex XAI algorithms like LIME or SHAP directly on the resource-constrained sensor nodes is impractical. It would impose prohibitive latency, consume excessive power, and likely exceed the available memory.
Therefore, the architectural solution is to offload the XAI processing to the gateway node. The gateway, being a dedicated, centralized, and likely mains-powered device, has the necessary computational headroom to perform these more intensive analyses without compromising the real-time performance of the sensor nodes. This architectural split is a direct consequence of the system's functional requirements meeting the hardware's physical constraints.


7.2 A Practical XAI Implementation Strategy


The XAI process is not continuous; it is triggered only when necessary, making it an efficient use of the gateway's resources.
                        * Step 1: Triggering the XAI Process: The gateway continuously monitors the status messages from all sensor nodes. The XAI routine is only initiated when a message arrives with the is_anomalous flag set to true.
                        * Step 2: Requesting Anomaly Data: Upon receiving an anomaly alert from a specific node (e.g., Node #3), the gateway's primary task is to acquire the data that caused the alert. It does this by sending a request message back to the anomalous node. This requires a two-way ESP-NOW communication link. The gateway uses the MAC address of the sender (which is provided in the OnDataRecv callback) to target the message specifically to that node. The message can be a simple structure requesting the raw feature data (the FFT output vector) that led to the high reconstruction error.
                        * Step 3: Implementing a Feature-Based Explanation: For a microcontroller-based gateway, the most practical and achievable XAI method is a form of feature importance explanation. This approach provides a direct and understandable reason for the anomaly without the extreme computational overhead of methods like LIME or SHAP. The implementation on the gateway would be as follows:
                        1. The gateway must have a baseline "normal" profile stored in its memory. This profile would be an array representing the average FFT feature vector calculated from all the normal data during the offline training phase.
                        2. When the gateway receives the anomalous FFT feature vector from the sensor node, it performs a direct comparison against the stored normal profile.
                        3. It calculates the absolute difference for each corresponding frequency bin in the two vectors.
                        4. It then identifies the top N (e.g., top 3) frequency bins with the largest differences. These are the features that have deviated most significantly from the norm.
                        5. This analysis provides a concrete, rule-based explanation.24 For example, the system can now generate a message like: "Anomaly on Motor 3: Significant deviation detected in the 120-150 Hz frequency band." This is far more actionable than a simple alert, as specific frequency bands can often be correlated with known mechanical failure modes (e.g., bearing wear, shaft misalignment).


7.3 Visualizing and Communicating Explanations


The final step is to present this explanation to the user in a clear and timely manner. The gateway can be connected to a simple display peripheral, such as a 1.3-inch OLED screen or a small TFT LCD.
Upon detecting and explaining an anomaly, the gateway's firmware would format and display a human-readable message on the screen.23 For example:






--------------------
SYSTEM ALERT
--------------------
MOTOR 2: ANOMALOUS
REASON: High-Frequency Vibration
BANDS: 135Hz, 145Hz

This provides the instant, local, and actionable feedback that is the core objective of integrating XAI into an industrial monitoring system. It empowers the user to make an informed decision immediately, fulfilling the promise of building a trustworthy AI partner.60


7.4 Advanced Option: Exploring LIME on the Gateway


For a student seeking a significant challenge, exploring a lightweight implementation of LIME on the gateway is a viable "stretch goal." This would represent a significant step up in explanation quality.
                        * Feasibility: While computationally expensive, a carefully optimized version of LIME could potentially run on a powerful MCU like an ESP32.32
                        * Implementation Sketch:
                        1. The gateway would need to have a copy of the same TFLite autoencoder model running on the sensor nodes.
                        2. Upon receiving anomalous FFT data, the gateway would programmatically generate a small number of perturbed samples around this data point in its own memory.
                        3. It would then run inference on its local copy of the model for each perturbed sample.
                        4. Finally, it would fit a simple linear model to these results to calculate the feature importance scores (the LIME explanation).
This process would be much slower than the simple feature comparison method but would provide a more nuanced, model-faithful explanation of the anomaly. The following table summarizes the trade-offs involved in selecting an XAI method for the gateway.
Table 2: XAI Method Trade-offs for the Gateway Node


XAI Method
	Computational Cost
	Memory Footprint
	Explanation Quality & Granularity
	Implementation Feasibility on Gateway (e.g., ESP32)
	Feature Importance Plot
	Very Low. Involves simple array comparisons against a stored baseline. The logic is straightforward and executes quickly.38
	Very Low. Only requires storing a single baseline "normal" feature vector in memory.
	Low. This method is highly interpretable but offers low granularity. It identifies which features are anomalous but does not explain their complex interactions or their specific contribution to the model's output score. It provides the "what" but not a deep "why".39
	High. This is the most practical and highly recommended starting point for the project. It is easily implemented in C++ and provides immediate, valuable diagnostic information with minimal resource impact.
	LIME (Local Interpretable Model-agnostic Explanations)
	High. The primary cost comes from generating dozens or even hundreds of perturbed data samples and then running a full model inference for each one. This is followed by the overhead of fitting a local surrogate model (e.g., linear regression).32
	High. The gateway must allocate memory to store the entire set of perturbed data samples and their corresponding model predictions, as well as the parameters for the surrogate model.
	Medium to High. LIME provides a local, linear approximation of the complex model's decision boundary. It explains the weighted contribution of each feature to a specific anomalous prediction, offering a much more nuanced explanation than a simple feature plot.26
	Challenging but Feasible. This is an excellent "stretch goal." Success would require careful optimization: using a minimal number of perturbations, a highly efficient linear solver, and accepting that the explanation process may introduce noticeable latency (seconds rather than milliseconds).
	SHAP (SHapley Additive exPlanations)
	Very High. SHAP's theoretical guarantees come from its need to evaluate feature coalitions, a process that is computationally explosive. A single explanation can require thousands of model evaluations, making it orders of magnitude more expensive than LIME.35
	Very High. Requires significant memory to manage the sampling processes, feature coalitions, and intermediate calculations needed to compute the Shapley values.
	Very High. SHAP is widely considered the "gold standard" for explanation quality, providing theoretically sound, consistent, and robust feature attributions that are additive and locally accurate.36
	Very Low / Impractical. Given the current state of microcontroller technology, implementing SHAP for real-time explanation on a device like an ESP32 is generally considered out of scope. Its computational and memory demands far exceed the available resources.
	

Part III: Evaluation, Extension, and Concluding Insights




Section 8: System Evaluation and Interpretation of Results


A cornerstone of any engineering project is a rigorous evaluation of its performance. For a complex, multi-faceted system like this one, evaluation must also be multi-faceted, addressing the performance of the machine learning model, the reliability of the network, and the quality of the explanations. This section provides a framework for the student to scientifically validate their work and interpret the results.


8.1 Anomaly Detection Performance Metrics


The core function of the sensor node is its ability to accurately distinguish between normal and anomalous states. To evaluate the performance of the trained autoencoder model, a dedicated test dataset is required. This dataset should be held out from the training process and must contain a mix of "normal" data and various types of manually induced "anomalous" data (e.g., by attaching a small weight to a motor's fan to create an imbalance).
With this labeled test set, the following standard classification metrics can be calculated 10:
                           * Accuracy: The overall percentage of correct classifications. Accuracy = (TP + TN) / (TP + TN + FP + FN)
                           * Precision: Of all the instances the model flagged as anomalous, what percentage were actually anomalous? This measures the cost of false alarms. Precision = TP / (TP + FP)
                           * Recall (Sensitivity): Of all the actual anomalies in the dataset, what percentage did the model successfully detect? This measures the model's ability to not miss real faults. Recall = TP / (TP + FN)
                           * F1-Score: The harmonic mean of Precision and Recall, providing a single, balanced measure of the model's performance. F1 = 2 * (Precision * Recall) / (Precision + Recall)
Here, TP = True Positives (anomaly correctly identified), TN = True Negatives (normal correctly identified), FP = False Positives (normal misidentified as anomaly), and FN = False Negatives (anomaly missed). A high F1-Score is the primary goal for the ML model.


8.2 Network Performance Metrics


The reliability of the distributed system depends on the performance of the ESP-NOW communication link. Two key metrics should be measured:
                           * Latency: This is the time elapsed from a sensor node sending a status message to the gateway node receiving it. This can be measured by synchronizing the clocks of two devices and timestamping the send and receive events. Low latency is a key benefit of ESP-NOW and should be quantified.
                           * Packet Error Rate (PER): This measures the percentage of transmitted packets that are lost and never received by the gateway. This test should be run over a long duration (e.g., several hours) and under varying environmental conditions (e.g., with physical obstructions) to assess the network's real-world robustness.


8.3 XAI Evaluation Metrics


Evaluating the "goodness" of an explanation is more challenging than evaluating model accuracy, as it can be subjective. However, several quantitative and qualitative metrics can be used to provide a structured assessment of the XAI component.63
                           * Fidelity (or Descriptive Accuracy): This metric assesses how accurately the explanation reflects the underlying model's behavior. A practical way to measure this is with a perturbation-based test:
                           1. Use the XAI method to identify the top-k most important features for a given anomalous prediction.
                           2. "Remove" or neutralize these features from the input data.
                           3. Feed this modified data back into the ML model.
                           4. A high-fidelity explanation should cause the model's prediction to change significantly (e.g., the anomaly score should drop, or the classification should flip back to "normal"). If the prediction doesn't change, the explanation did not capture the true reasons for the model's decision.64
                           * Sparsity: A good explanation should be concise and easy for a human to understand. Sparsity measures this by counting the number of features used in the explanation. An explanation that highlights 3 key frequency bands is more useful and sparser than one that lists 50.63
                           * User Trust and Comprehensibility: This is a more qualitative metric, often assessed through user feedback. The ultimate goal of XAI is to make the system trustworthy and useful to a human operator. The student can evaluate this by demonstrating the system to a non-expert and asking questions like: "Does this explanation help you understand why the system raised an alert?" and "Does this explanation increase your confidence in the system's decision?".60
It is crucial to recognize the inherent tension between these evaluation domains. A more complex and non-linear machine learning model might achieve a higher F1-Score, making it more accurate. However, this very complexity makes the model more opaque and harder to explain. A simple feature-based explanation might lose fidelity for such a model, forcing a move to a more computationally expensive XAI method like LIME. This creates a design trade-off: the "best" overall system is not necessarily the one with the highest possible accuracy, but rather the one that achieves sufficient accuracy while remaining efficiently explainable within the hardware constraints of the gateway. This balancing act is a key lesson in modern embedded AI engineering.


Section 9: Future Work and Project Extensions


This project, while comprehensive, serves as a robust foundation upon which many advanced features and capabilities can be built. The following extensions represent logical next steps for a student wishing to explore the concepts further.
                           * Cloud Integration and Remote Monitoring: The gateway node is perfectly positioned to act as a bridge to the cloud. By implementing the MQTT protocol on the gateway, it can publish anomaly alerts and their corresponding explanations to a cloud-based IoT platform like Ubidots, Thingspeak, or AWS IoT Core.48 This would enable long-term data logging, the creation of historical performance dashboards, and the ability to send remote notifications (e.g., email or SMS alerts) to maintenance personnel.
                           * On-Device Model Updates: A significant challenge in deployed ML systems is updating the model with new data. A powerful extension would be to explore methods for updating the autoencoder model on the gateway or even the sensor nodes without requiring a complete firmware re-flash. This touches upon the advanced field of continual learning, where a model can adapt to new "normal" conditions or learn from newly identified anomalies over time.
                           * Federated Learning for Enhanced Privacy: For a truly advanced exploration, the concept of federated learning could be investigated. In this paradigm, instead of sending data to a central gateway for analysis, the sensor nodes themselves could collaboratively train a shared global model. Each node would train on its local data and only share the resulting model updates (gradients), not the raw data itself. A central server would aggregate these updates to improve the global model, which is then pushed back to the nodes. This approach offers unparalleled privacy and is a major area of research for distributed AI systems.4
                           * Multi-Modal Anomaly Detection: The robustness of the anomaly detection can be significantly enhanced by using multiple types of sensors, a technique known as data fusion.30 An extension could involve adding a microphone to each sensor node alongside the accelerometer. The firmware would then extract features from both the vibration and acoustic data (e.g., FFT for vibration, MFCCs for audio). These features would be concatenated and fed into a larger autoencoder model. This multi-modal approach can detect a wider range of faults, as some issues may be more apparent in the acoustic signature while others are more apparent in the vibration pattern.


Section 10: Conclusion: Key Takeaways for the Aspiring Embedded AI Engineer


This report has laid out a blueprint for a capstone-level project that stands at the confluence of several cutting-edge domains in electronics and communication engineering. The successful implementation of this distributed, explainable anomaly detection network equips the student with a skill set that is not only academically rigorous but also highly relevant to the future of the industry.
The key takeaways from this endeavor transcend the specific application of motor monitoring. They represent a new way of thinking about embedded systems design:
                           1. The Shift from Programming to Training: The project demonstrates a move away from purely deterministic, rule-based firmware towards a data-driven approach. Instead of trying to manually code for every possible fault condition, the engineer trains a model to learn the complex patterns of normal behavior from real-world data.
                           2. Systems Thinking is Paramount: This is not a project about a single device, but about a distributed system. The architecture, with its division of labor between low-power sensor nodes and a more capable gateway, is a direct result of balancing competing requirements: real-time response, power efficiency, and the computational demands of advanced algorithms like XAI.
                           3. Intelligence Requires Efficiency: The principles of TinyML—model optimization, quantization, and efficient inference engines—are not just theoretical concepts. They are the practical tools that make it possible to deploy non-trivial AI capabilities on cost-effective, low-power hardware.
                           4. Explainability is a Functional Requirement: The integration of XAI is perhaps the most forward-looking aspect of this project. It underscores the understanding that for AI systems to be adopted and relied upon, especially in critical applications, they cannot be opaque black boxes. Building systems that are not only intelligent but also transparent and trustworthy is rapidly becoming a non-negotiable requirement for the modern engineer.
By navigating the challenges of real-time sensing, on-device machine learning, low-power networking, and explainable AI, the student who undertakes this project will not just be completing a degree requirement. They will be building a microcosm of the next generation of intelligent embedded systems—systems that are distributed, autonomous, efficient, and, most importantly, designed to collaborate effectively with their human users. This is the future of the intelligent edge.
Works cited
                           1. Unlocking Edge Computing in Microcontrollers - Number Analytics, accessed on July 27, 2025, https://www.numberanalytics.com/blog/ultimate-guide-edge-computing-microcontrollers
                           2. Edge Computing Tutorial - Tutorials Point, accessed on July 27, 2025, https://www.tutorialspoint.com/edge-computing/index.htm
                           3. Edge Computing Tutorial [ REAL-TIME ] Examples | ACTE | Updated ..., accessed on July 27, 2025, https://www.acte.in/edge-computing-tutorial
                           4. Edge AI: Deploying Machine Learning on Resource- Constrained Devices - ResearchGate, accessed on July 27, 2025, https://www.researchgate.net/publication/390208735_Edge_AI_Deploying_Machine_Learning_on_Resource-_Constrained_Devices
                           5. EDGE AI for Real-Time Monitoring - YouTube, accessed on July 27, 2025, https://www.youtube.com/watch?v=2Hvq5AwPJdU
                           6. Introduction to TinyML on Microcontrollers: Bringing AI to the Edge - ThinkRobotics.com, accessed on July 27, 2025, https://thinkrobotics.com/blogs/learn/introduction-to-tinyml-on-microcontrollers-bringing-ai-to-the-edge
                           7. Top 10 Edge AI Hardware for 2025 - Jaycon | Product Design, PCB & Injection Molding, accessed on July 27, 2025, https://www.jaycon.com/top-10-edge-ai-hardware-for-2025/
                           8. A TinyML Journey: Neural Networks on Cheap, Tiny Embedded Microcontrollers - Medium, accessed on July 27, 2025, https://medium.com/@traiano_1008/a-tinyml-journey-neural-networks-on-cheap-tiny-embedded-microcontrollers-020ea0f30c44
                           9. SMART SENSORS FOR ANOMALY DETECTION IN IoT ARCHITECTURE DRIVEN BY TINYML - ResearchGate, accessed on July 27, 2025, https://www.researchgate.net/publication/373829673_SMART_SENSORS_FOR_ANOMALY_DETECTION_IN_IoT_ARCHITECTURE_DRIVEN_BY_TINYML
                           10. JOURNAL OF MARITIME RESEARCH Lightweight Unsupervised Model for Anomaly Detection on Microcontroller Platforms, accessed on July 27, 2025, https://www.jmr.unican.es/jmr/article/download/874/863
                           11. TinyML: Running Deep Learning Models on Microcontrollers | by Sucheta Mandal - Medium, accessed on July 27, 2025, https://medium.com/@sucheta963/tinyml-running-deep-learning-models-on-microcontrollers-a1524a69e98c
                           12. TinyML: Enabling of Inference Deep Learning Models on Ultra-Low-Power IoT Edge Devices for AI Applications - PMC, accessed on July 27, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC9227753/
                           13. ShawnHymel/tinyml-example-anomaly-detection - GitHub, accessed on July 27, 2025, https://github.com/ShawnHymel/tinyml-example-anomaly-detection
                           14. Accelerometer-Based Motion Recognition and Alerting System for Abnormal Activity Detection in Dynamic Environments, accessed on July 27, 2025, https://internationalpubls.com/index.php/cana/article/download/5218/2938/9153
                           15. 10 Best Edge Computing Project Ideas for Beginners [With Code], accessed on July 27, 2025, https://www.placementpreparation.io/blog/edge-computing-project-ideas-for-beginners/
                           16. Mechanical Anomaly Detection on an Embedded Microcontroller | Request PDF, accessed on July 27, 2025, https://www.researchgate.net/publication/361476876_Mechanical_Anomaly_Detection_on_an_Embedded_Microcontroller
                           17. Anomaly Detection Technique in Sound to Detect Faulty Equipment ..., accessed on July 27, 2025, https://www.ntt-review.jp/archive/ntttechnical.php?contents=ntr201708fa5_s.html
                           18. An Auto-Encoder Based TinyML Approach for Real-Time Anomaly Detection, accessed on July 27, 2025, https://www.researchgate.net/publication/364182863_An_Auto-Encoder_Based_TinyML_Approach_for_Real-Time_Anomaly_Detection
                           19. Autoencoder: Anomaly Detection for Vibration Data. | by Cendikia Ishmatuka - Medium, accessed on July 27, 2025, https://cendikiaishmatuka.medium.com/autoencoder-anomaly-detection-for-vibration-data-6c1dff82fd43
                           20. TinyML: Implementing FFT on Microcontrollers for Vibration Monitoring - Patsnap Eureka, accessed on July 27, 2025, https://eureka.patsnap.com/article/tinyml-implementing-fft-on-microcontrollers-for-vibration-monitoring
                           21. Vibration Analysis: FFT, PSD, and Spectrogram Basics [Free Download] - enDAQ Blog, accessed on July 27, 2025, https://blog.endaq.com/vibration-analysis-fft-psd-and-spectrogram
                           22. Unsupervised Multi-sensor Anomaly Localization with Explainable AI - DFKI, accessed on July 27, 2025, https://www.dfki.de/fileadmin/user_upload/import/12399_AIAI2022_646_MinaAmeli.pdf
                           23. Explainable Anomaly Detection System for Categorical Sensor Data in Internet of Things - GitLab, accessed on July 27, 2025, https://haifengchen.gitlab.io/intro/papers/ECML22_PYuan.pdf
                           24. Explainable Anomaly Detection with RuleFit: An Intuitive Guide | Towards Data Science, accessed on July 27, 2025, https://towardsdatascience.com/explainable-anomaly-detection-with-rulefit-an-intuitive-guide/
                           25. (PDF) Optimized Tiny Machine Learning and Explainable AI for Trustable and Energy-Efficient Fog-Enabled Healthcare Decision Support System - ResearchGate, accessed on July 27, 2025, https://www.researchgate.net/publication/383673537_Optimized_Tiny_Machine_Learning_and_Explainable_AI_for_Trustable_and_Energy-Efficient_Fog-Enabled_Healthcare_Decision_Support_System
                           26. Unlocking Transparency in IoT with XAI - Number Analytics, accessed on July 27, 2025, https://www.numberanalytics.com/blog/explainable-ai-in-iot-embedded-systems
                           27. The Future of IoT: XAI for Smarter Devices - Number Analytics, accessed on July 27, 2025, https://www.numberanalytics.com/blog/future-of-iot-with-explainable-ai
                           28. XAI: Explainable Artificial Intelligence - DARPA, accessed on July 27, 2025, https://www.darpa.mil/research/programs/explainable-artificial-intelligence
                           29. A Survey on Explainable Anomaly Detection - arXiv, accessed on July 27, 2025, https://arxiv.org/pdf/2210.06959
                           30. XAI for Industry 5.0 -Concepts, Opportunities, Challenges and Future Directions, accessed on July 27, 2025, https://www.researchgate.net/publication/384648798_XAI_for_Industry_50_-Concepts_Opportunities_Challenges_and_Future_Directions
                           31. Interpretable Machine Learning - Christoph Molnar, accessed on July 27, 2025, https://christophm.github.io/interpretable-ml-book/
                           32. Explain your model predictions with LIME - Kaggle, accessed on July 27, 2025, https://www.kaggle.com/code/prashant111/explain-your-model-predictions-with-lime
                           33. Unleash the Black Box: A Deep Dive into LIME for Making AI Understandable - Aravind Kolli, accessed on July 27, 2025, https://aravindkolli.medium.com/unleash-the-black-box-a-deep-dive-into-lime-for-making-ai-understandable-3a2442987608
                           34. Explainable AI in Edge Devices: A Lightweight Framework for Real-Time Decision Transparency | International Journal of Engineering and Computer Science - ijecs, accessed on July 27, 2025, https://ijecs.in/index.php/ijecs/article/view/5181
                           35. Explainable AI in Edge Devices: A Lightweight Framework for Real-Time Decision Transparency - ResearchGate, accessed on July 27, 2025, https://www.researchgate.net/publication/393592059_Explainable_AI_in_Edge_Devices_A_Lightweight_Framework_for_Real-Time_Decision_Transparency
                           36. [Literature Review] Pruning-Based TinyML Optimization of Machine Learning Models for Anomaly Detection in Electric Vehicle Charging Infrastructure - Moonlight, accessed on July 27, 2025, https://www.themoonlight.io/review/pruning-based-tinyml-optimization-of-machine-learning-models-for-anomaly-detection-in-electric-vehicle-charging-infrastructure
                           37. Building Efficient ML: From No‑Code AutoML to TinyML on Edge Devices | by Vignesh Selvaraj | Jul, 2025 | Medium, accessed on July 27, 2025, https://medium.com/@vignesh-selvaraj/building-efficient-ml-from-no-code-automl-to-tinyml-on-edge-devices-16cc294f4f23
                           38. Motion recognition + anomaly detection | Edge Impulse ..., accessed on July 27, 2025, https://docs.edgeimpulse.com/docs/tutorials/end-to-end-tutorials/time-series/continuous-motion-recognition
                           39. Using Accelerometer and GPS Data for Real-Life Physical Activity Type Detection - MDPI, accessed on July 27, 2025, https://www.mdpi.com/1424-8220/20/3/588
                           40. Developing a Fire Monitoring System Based on MQTT, ESP-NOW, and a REM in Industrial Environments - MDPI, accessed on July 27, 2025, https://www.mdpi.com/2076-3417/15/2/500
                           41. ESP-NOW: Receive Data from Multiple ESP32 Boards (many-to-one ..., accessed on July 27, 2025, https://randomnerdtutorials.com/esp-now-many-to-one-esp32/
                           42. ESP-NOW Wireless Communication Protocol - Espressif Systems, accessed on July 27, 2025, https://www.espressif.com/en/solutions/low-power-solutions/esp-now
                           43. Anomaly detection and security threats in Internet of Things: A study of data integrity in the measurement process, accessed on July 27, 2025, https://www.astrj.com/pdf-204321-125456?filename=Anomaly%20detection%20and.pdf
                           44. Study on Supervised Anomaly Detection Model For MQTT-Based lot Data for Dos Attacks - IJFMR, accessed on July 27, 2025, https://www.ijfmr.com/papers/2025/2/39144.pdf
                           45. Accelerometers in Microcontrollers - Number Analytics, accessed on July 27, 2025, https://www.numberanalytics.com/blog/ultimate-guide-accelerometers-microcontrollers
                           46. Nano 33 BLE Sense | Arduino Documentation, accessed on July 27, 2025, https://www.arduino.cc/en/Guide/NANO33BLESense
                           47. Audio-based Anomaly Detection in Industrial Machines Using Deep One-Class Support Vector Data Description - arXiv, accessed on July 27, 2025, https://arxiv.org/html/2412.10792v1
                           48. Development of Wireless Smart Current Sensor for Power Monitoring System, accessed on July 27, 2025, https://www.researchgate.net/publication/376948642_Development_of_Wireless_Smart_Current_Sensor_for_Power_Monitoring_System
                           49. AI:How to create a current sensing classifier using NanoEdge AI ..., accessed on July 27, 2025, https://wiki.st.com/stm32mcu/wiki/AI:How_to_create_a_current_sensing_classifier_using_NanoEdge_AI_Studio
                           50. Electrical Fault Detection System Using Arduino for Household Appliances - Journal of Engineering Sciences, accessed on July 27, 2025, https://jespublication.com/uploads/2024-V15I40210.pdf
                           51. ESP32 vs Raspberry Pi : Definition & the Main Differences - ElProCus, accessed on July 27, 2025, https://www.elprocus.com/difference-between-esp32-vs-raspberry-pi/
                           52. ESP32 vs Arduino vs Raspberry Pi Pico: Which is Better ..., accessed on July 27, 2025, https://openelab.io/blogs/learn/esp32-vs-arduino-vs-raspberry-pi-pico-which-is-better
                           53. Nano 33 BLE Sense Rev2 - Arduino Documentation, accessed on July 27, 2025, https://docs.arduino.cc/hardware/nano-33-ble-sense-rev2/
                           54. Should I Switch from ESP8266/ESP32 to Pico W? - Raspberry Pi Forums, accessed on July 27, 2025, https://forums.raspberrypi.com/viewtopic.php?t=349875
                           55. Identify Shapes Using Machine Learning on Arduino Nano 33 BLE Sense Hardware, accessed on July 27, 2025, https://www.mathworks.com/help/matlab/supportpkg/identify-shapes-using-machine-learning-arduino-nano33-ble.html
                           56. Sensor data transmission from node to hub using espnow - ESP32 Forum, accessed on July 27, 2025, https://esp32.com/viewtopic.php?t=5172
                           57. FFT on vibration analysis - MATLAB Answers, accessed on July 27, 2025, https://www.mathworks.com/matlabcentral/answers/558-fft-on-vibration-analysis
                           58. ESPNow Protocol-Based IIoT System for Remotely Monitoring and Controlling Industrial Systems - Journal UMY, accessed on July 27, 2025, https://journal.umy.ac.id/index.php/jrc/article/download/21925/9567/89101
                           59. Towards Explainable Artificial Intelligence (XAI): A Data Mining Perspective - arXiv, accessed on July 27, 2025, https://arxiv.org/html/2401.04374v2
                           60. Evaluating machine-generated explanations: a “Scorecard” method for XAI measurement science - Frontiers, accessed on July 27, 2025, https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2023.1114806/full
                           61. Enhancing IoT Network Security through Adaptive Curriculum Learning and XAI - arXiv, accessed on July 27, 2025, https://arxiv.org/html/2501.11618v1
                           62. Pruning-Based TinyML Optimization of Machine Learning ... - arXiv, accessed on July 27, 2025, https://arxiv.org/abs/2503.14799
                           63. Evaluation Metrics Research for Explainable Artificial Intelligence Global Methods Using Synthetic Data - MDPI, accessed on July 27, 2025, https://www.mdpi.com/2571-5577/6/1/26
                           64. On Evaluating Black-Box Explainable AI Methods for Enhancing Anomaly Detection in Autonomous Driving Systems - PubMed Central, accessed on July 27, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC11175219/
                           65. XAI evaluation metrics | Download Scientific Diagram - ResearchGate, accessed on July 27, 2025, https://www.researchgate.net/figure/AI-evaluation-metrics_fig3_365718190